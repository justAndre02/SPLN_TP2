{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')\n",
    "        self.stopwords = set(stopwords)\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        line = line.lower()\n",
    "        tokens = word_tokenize(line)\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        return tokens\n",
    "\n",
    "    def fetch_documents(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            records = json.load(file)\n",
    "        return records\n",
    "\n",
    "    def preprocess_notes(self, records):\n",
    "        notes = [self.preprocess(record['notes']) for record in records]\n",
    "        return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"pt_core_news_sm\")\n",
    "        self.nlp.max_length = 10000000 \n",
    "\n",
    "    def get_named_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        entities = [(ent.text, ent.start_char) for ent in doc.ents]\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPairCreator:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.answer_extractor = AnswerExtractor()\n",
    "\n",
    "    def create_qa_pairs(self, documents):\n",
    "        qa_pairs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            document_id = doc['id']\n",
    "            text = doc['notes']\n",
    "\n",
    "            # Fetch the full text of the document\n",
    "            conn = psycopg2.connect(database=\"diariorepublica\",\n",
    "                                    user=\"postgres\",\n",
    "                                    host='localhost',\n",
    "                                    password=\"1597535\",\n",
    "                                    port=5432)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(f\"SELECT html_text FROM public.dreapp_documenttext WHERE document_id = {document_id}\")\n",
    "            result = cur.fetchone()\n",
    "            if result is not None:\n",
    "                full_text = result[0]\n",
    "            else:\n",
    "                full_text = \"\"\n",
    "\n",
    "            conn.close()\n",
    "\n",
    "            # Generate candidate answers using NER\n",
    "            entities = self.answer_extractor.get_named_entities(full_text)\n",
    "\n",
    "            # Define questions (in Portuguese)\n",
    "            questions = [\n",
    "                \"Qual é o tema principal do documento?\",\n",
    "                \"Quem é o autor ou a autoridade emissora do documento?\",\n",
    "                \"Quando o documento foi publicado?\",\n",
    "                \"Quais ações ou medidas são propostas no documento?\",\n",
    "                \"Quais organizações ou indivíduos são mencionados no documento?\",\n",
    "                \"Quais locais ou regiões são referenciados no documento?\",\n",
    "                \"Qual é o propósito do documento?\"\n",
    "            ]\n",
    "\n",
    "            # Match questions with identified entities to create QA pairs\n",
    "            for question in questions:\n",
    "                for entity, start_char in entities:\n",
    "                    qa_pairs.append({\n",
    "                        \"context\": full_text,\n",
    "                        \"question\": question,\n",
    "                        \"answers\": {\"text\": [entity], \"answer_start\": [start_char]}\n",
    "                    })\n",
    "\n",
    "        return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_System:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.qa_pair_creator = QAPairCreator()\n",
    "        self.generator = pipeline('question-answering', model='pierreguillou/bert-base-cased-squad-v1.1-portuguese')\n",
    "\n",
    "    def preprocess_and_create_qa_pairs(self, file_path):\n",
    "        records = self.preprocessor.fetch_documents(file_path)\n",
    "        qa_pairs = self.qa_pair_creator.create_qa_pairs(records)\n",
    "        return qa_pairs\n",
    "\n",
    "    def answer_question(self, question, file_path):\n",
    "        # Preprocess the question and convert it to a vector\n",
    "        question_tokens = self.preprocessor.preprocess(question)\n",
    "        dictionary = Dictionary(self.preprocessor.preprocess_notes(self.preprocessor.fetch_documents(file_path)))\n",
    "        question_bow = dictionary.doc2bow(question_tokens)\n",
    "\n",
    "        # Create a dictionary and TF-IDF model\n",
    "        corpus = [dictionary.doc2bow(note) for note in self.preprocessor.preprocess_notes(self.preprocessor.fetch_documents(file_path))]\n",
    "        tfidf = TfidfModel(corpus, normalize=True)\n",
    "\n",
    "        # Calculate the cosine similarity between the question and each note\n",
    "        index = SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
    "        similarities = index[tfidf[question_bow]]\n",
    "\n",
    "        # Get the index of the most similar note\n",
    "        most_similar_index = similarities.argmax()\n",
    "        records = self.preprocessor.fetch_documents(file_path)\n",
    "        print(f\"Most similar document: {records[most_similar_index]['id']}\")\n",
    "\n",
    "        # Fetch the full text of the most similar document\n",
    "        conn = psycopg2.connect(database=\"diariorepublica\",\n",
    "                                user=\"postgres\",\n",
    "                                host='localhost',\n",
    "                                password=\"1597535\",\n",
    "                                port=5432)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT html_text FROM public.dreapp_documenttext WHERE document_id = {records[most_similar_index]['id']}\")\n",
    "        result = cur.fetchone()\n",
    "        conn.close()\n",
    "\n",
    "        full_text = result[0] if result else \"\"\n",
    "\n",
    "        # Generate an answer based on the full text\n",
    "        answer = self.generator(question=question, context=full_text)\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "file_path  = \"documentos.json\"\n",
    "qa_system = QA_System()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15736\\1155210340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqa_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqa_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_and_create_qa_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15736\\1734694838.py\u001b[0m in \u001b[0;36mpreprocess_and_create_qa_pairs\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_and_create_qa_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mqa_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqa_pair_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_qa_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mqa_pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15736\\2789368633.py\u001b[0m in \u001b[0;36mcreate_qa_pairs\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Generate candidate answers using NER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manswer_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_named_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Define questions (in Portuguese)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15736\\3918757432.py\u001b[0m in \u001b[0;36mget_named_entities\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_named_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\.conda\\envs\\myenv\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_pairs = qa_system.preprocess_and_create_qa_pairs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_system.answer_question(\"O que é aconteceu no dia 11 de setembro de 2001?\", file_path)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
